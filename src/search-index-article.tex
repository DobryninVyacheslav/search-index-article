\documentclass[
    twocolumn,
%    hf, %enable header and footer.
]{template/ceurart}

\sloppy
\usepackage{listings}
\usepackage{svg}
\usepackage{underscore} % for normal display of underscore on svg images
\lstset{breaklines=true}

\begin{document}
    \copyrightyear{2022}
    \copyrightclause{Copyright for this paper by its authors.
    Use permitted under Creative Commons License Attribution 4.0
    International (CC BY 4.0).}
    \conference{This command is for the conference information}
    \title{Optimization of the structure of a full-text search index using neural networks of the "Transformer" architecture}
    \author[1]{Vyacheslav YU. Dobrynin}
    \author[1]{Roman K. Abramovich}
    \author[1]{Artem D. Gorshkov}
    \author[1]{Alexey V. Platonov}
    \address[1]{ITMO University, Kronverksky Pr. 49, bldg. A, Saint-Petersburg, 197101, Russian Federation}
    \begin{abstract}
        The use of deep neural networks in information retrieval significantly improves its effectiveness,
        but negatively affects the performance of the process.
        To deal with this, we propose a new ranking model that uses the deep neural network
        of the ``Transformer'' architecture (in particular, BERT) for efficient information retrieval.

        In accordance with the proposed approach, contextualized vector representations are extracted
        from documents during indexing, after which these representations are clustered for each independent token.
        The resulting clusters reflect different meanings of the words and are indirectly used as inverted index keys.
        The values are the documents in which these contextualized meanings of the words occur,
        and the distance from this document to the contextualized embedding
        is also stored along with information about the document.
        Thus, after indexing, we get an index that contains pre-calculated distances
        from the contextualized meanings of dictionary elements to documents,
        which allows us to avoid performance losses in calculating the distance at the online.
        At the search stage, the query is transformed into a set of contextualized vectors representing
        each query token, which allows us to use these vectors to retrieve most semantically close neighbor-tokens
        and use them to extract relevant documents from the index.
        This way of searching for contextualized embeddings solves the problem of vocabulary mismatch,
        as well as the problem of having polysemous words in the vocabulary.
    \end{abstract}
    \begin{keywords}
        Search engine \sep transformer \sep BERT \sep inverted index \sep optimization \sep vocabulary mismatch
        \sep word sense disambiguation.
    \end{keywords}
    \maketitle


    \section{Introduction}
    Pre-trained neural networks make it possible to effectively use complex models with less effort
    to train them to solve various kinds of problems.
    Information retrieval is no exception, in this area for a long time simple algorithms
    were used that showed high performance, but relatively low quality of search.
    The use of machine learning makes it possible to significantly improve the quality
    characteristics of the search, but due to the need to perform a large number of operations,
    i.e. high algorithmic complexity, this leads to performance losses~\cite{performanceLoss}.
    In this regard, modern search engines use two stages of retrieving relevant documents~\cite{multiStageRetrieval}.
    At the first stage, simple and effective algorithms are used that screen out most of the candidates,
    and at the second stage, neural networks are used to more accurately determine
    the relevance of the remaining documents.
    Thus, the speed of simple algorithms and the quality of complex ones are combined.
    But such an architecture has drawbacks: the documents obtained at the first stage are retrieved
    without understanding the semantics of the texts, and potentially relevant documents are skipped
    due to a vocabulary mismatch between the query and documents dictionaries~\cite{vocabularyMismatch1,vocabularyMismatch2}.

    The paper presents an approach to building an inverted index of a search engine using a neural network
    of the ``Transformer'' architecture, aimed at overcoming the above disadvantages.
    The paper also considers the solution to the problem of using polysemous words by considering their context.
    The peculiarity of the approach is that the distances from the elements of the dictionary,
    taking into account the context, to the documents are calculated at the indexing stage using the neural model,
    and the search is performed by the index without it, as a result of which the search is performed quickly
    and efficiently based on the semantics of the text.


    \section{Related Work}

    \paragraph{Vector Representation.}
    The work presented by Zamani \textit{et al.} introduced SNRM~\cite{snrm},
    an approach for mapping documents to sparse vector representations.
    This allows the use of an inverted index, resulting in high search speed and comparatively low memory consumption.
    However, in practice, it turned out that the effectiveness of SNRM is significantly
    inferior to modern state-of-the-art approaches.

    Unlike SNRM, we use dense vector BERT-based representations.
    However, they also have their limitations.
    So, for their search, nearest neighbor algorithms with linear complexity can be used,
    which is unacceptable for large amounts of data used in information retrieval.
    An acceptable solution is to use algorithms of the approximate nearest neighbor (ANN) class~\cite{ann},
    which allow a small loss of accuracy with an increase in speed by several orders of magnitude.
    Their disadvantage, depending on the specific implementation,
    is either high memory consumption or suboptimal performance.
    In this work, we use the HNSW~\cite{hnsw} algorithm based on proximity graphs.

    \paragraph{BERT.}
    To convert a document into a semantic vector space, the BERT~\cite{devlin2018bert} model is used,
    which shows the state-of-the-art results in many NLP tasks.

    Models SparTerm~\cite{sparTerm} and it's successor SPLADE ~\cite{splade} also use BERT to build an inverted
    index with regard to token's contexts and vocabulary mismatch problem.

    They use BERT to create a sparse term importance distribution for the whole dictionary for each passage at the indexing time.
    They do so by extracting contextualized vectors for each token in the passage and computing its importance with every other
    token in the dictionary.

    Despite the fact that their approach solves the same problems as ours, it requires much more computational resources during indexing
    as BERT is used not only to extract contextualized vectors, but also to compare these vectors with the
    entire vocabulary.

    Authors of ~\cite{colbert} introduce a new concept called ``late interaction'' with their ColBERT model.
    It is based on deep contextualized embeddings of document and query tokens, but with the late interaction
    method they were able to pre-compute document's embeddings offline in order to provide an effective end-to-end retrieval model.

    Although our approach shares the same goal to pre-compute all the necessary document embeddings offline, we aim to
    build an inverted index as the output of our algorithm, while ColBERT relies on using dense vector-similarity search libraries,
    such as faiss ~\cite{faiss}, to find relative documents at a query processing time.
    That also means that using our approach all the term-document scores will be computed offline,
    while ColBERT still needs to compute scores between document and query online.

    \paragraph{BERT's optimization.}
    BERT model is computationally expensive.
    Many researchers optimize it using distilling~\cite{bertDistilling},
    compression~\cite{bertCompression}, and pruning~\cite{bertPruning}.
    We decided to go by changing the architecture, so that BERT is not used to search for
    similarity score between a query and a document online,
    but is used to calculate it when building the inverted index.
    Using BERT online is only needed to find contextualized query token vectors,
    which will not significantly impact performance due to small query sizes.
    Moreover, to speed up this process, we use DistilBERT, which is 40\% smaller, 60\% faster,
    and retains 97\% of the language understanding capabilities compared to the base BERT~\cite{bertDistilling}.

    \paragraph{Contextual Word Sense Disambiguation.}
    There are many polysemous words in document and query dictionaries.
    The different meanings of a word depend on the contexts in which the word is used and
    can be distinguished with their help.
    There are methods that assign different vector representations
    to polysemous words depending on their context~\cite{athiwaratkun2018probabilistic}.
    This is also used in our algorithm, the keys of the inverted index will not be tokens,
    but the contextualized senses of these tokens.
    Thus, documents from the inverted index will be retrieved not by a specific token,
    but by its sense related to a specific context, which is meant in the query.


    \section{Inverted Index Construction}
    The proposed approach is based on using BERT to find similarities between a query and a document.
    This model helps to represent a document or a query in a multidimensional semantic space,
    using which one can find the distance or otherwise similarity between a document and a query,
    taking into account the meaning of the texts.

    However, the use of deep neural networks is accompanied by performance losses,
    which can be critical in the direct interaction of the user with the search engine.
    In view of this, a distinctive feature of the proposed approach is that it involves the use of a neural network
    only during the construction of the index.
    This is achieved due to the special structure of the inverted index,
    which makes it possible to store all the information necessary for searching.
    Also, the index is built in such a way that documents are searched for by contextualized meanings of words.
    This process is carried out in several stages.

    \subsection{Getting Contextualized Vector Representations}
    The first stage is preparatory.
    On it, for all tokens of each document obtained after preprocessing,
    windows of a given size are extracted, which are tokens and their contexts.
    So for a window length of 3 and the sentence ``ITMO University is located in Saint-Petersburg'',
    the sets will be as follows: [itmo, university], [itmo, university, locate],
    [university, locate, saint-petersburg], [locate, saint-petersburg].
    Next, the windows are passed to the BERT model to find their vector representations.
    The resulting contextualized vectors are stored in the key-value database
    along with the ID of the document to which the passed window belongs.
    Pairs (document ID, vector) for the same token are appended, not replaced.
    This completes the preparatory phase.
    Figure~\ref{fig:storeVectors} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/getting-contextualized-vectors}
        \caption{Preparation of vector representations}
        \label{fig:storeVectors}
    \end{figure}

    This stage is auxiliary and its results are necessary for the subsequent stages.

    \subsection{HNSW Index Building}
    The contextualized vectors obtained at the previous stage for each token represent its meaning in some context.
    However, it is highly likely that some vectors will represent the same meaning, and to be more precise,
    the vectors will be divided into groups in the semantic space,
    each of which will represent a specific context value of the token.
    To divide into such groups (clusters), hierarchical clustering is used in the work.
    The number of clusters equal to 16 was chosen empirically and will be refined during the experiments.
    Next, for the resulting clusters, their centroids are calculated.
    The centroid vector represents one of the contextualized token senses.
    After that, the token, the cluster identifier (which is the cluster serial number for a particular token),
    and the resulting centroid are stored in the HNSW index.
    Figure~\ref{fig:hnswIndexBuilding} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/hnsw-index-building}
        \caption{HNSW index building process}
        \label{fig:hnswIndexBuilding}
    \end{figure}

    The resulting HNSW index is needed both for building an inverted index and during an online search.

    \subsection{Inverted Index Building}
    This stage is the final one, on which the formation of the inverted index is performed.

    The vector representations of the contextual senses of the token obtained at the first stage
    are passed to the HNSW index built in the second stage to search for their nearest neighbors.
    The number of neighbors equal to 5 was chosen empirically and will be refined during the experiments.
    During the evaluation, a compromise will be chosen between the quality of the search and
    the speed of indexing and searching.
    The extracted neighbors are semantically close to the original contextual sense of the token,
    which means that they can also represent the sense of the document to which the token belongs.
    Although neighbors may contain different tokens, it is important that their semantic meaning be the same,
    as represented through embeddings.
    Thus, the vocabulary mismatch problem is solved.

    In the next step, distances are calculated between the vector representations of documents and
    the centroid vectors of the neighbors of the considered contextualized vectors.
    Distances are found using the dot product.
    The resulting scores, along with the document IDs, are written as values into an inverted index against the keys,
    which are (token, cluster ID) pairs.
    The process is repeated until the scores for all prepared contextualized vectors are indexed.
    Figure~\ref{fig:invertedIndexBuilding} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/inverted-index-building}
        \caption{Inverted index building process}
        \label{fig:invertedIndexBuilding}
    \end{figure}


    \section{Search Process}
    The search process starts in the same way as the indexing process.
    For each token of the input query obtained after preprocessing, windows of the fixed size are extracted.
    Then, each window is processed with the following algorithm:
    \begin{enumerate}
        \item Obtain vector representation by passing window to the BERT model;
        \item Get nearest neighbours from the HNSW index using obtained vector;
        \item For each pair (token, cluster ID) returned from HNSW, extract list of candidate-documents from inverted index;
        \item If a document is already present in a candidate list, merge it with the new candidate by summing their scores.
    \end{enumerate}

    At the second step of this algorithm, HNSW search can return tokens different,
    but semantically close to the original token from the query.
    Thus, the vocabulary mismatch problem is being solved both during indexing and search time.

    Next steps are no differ from the classical search algorithms.
    We sort the list of candidate-documents by their resulting score and return top-N documents as the searching result.

    Figure~\ref{fig:searchProcess} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/search-process}
        \caption{Search process for a given Query}
        \label{fig:searchProcess}
    \end{figure}


    \section{Methodology}\label{sec:methodology}

    \paragraph{Dataset.}
    Many information retrieval works use the MS MARCO~\cite{msMarco} dataset from Microsoft,
    which was released in 2016 and adapted for retrieval in 2018.
    The dataset contains 8.8 million web page passages and 3.2 million documents that were collected
    from Bing search engine logs.
    Each query is associated with a document or multiple documents by relevancy judgments.
    The development set contains 6980 requests.

    Thus, the dataset makes it possible to solve both the passage retrieval tasks and the document retrieval tasks.
    Thanks to this, the system is evaluated on real data, which means that the evaluation
    results will more objectively show how the search engine will work in production environment.

    It is also important to note that this dataset is often used by researchers in information retrieval,
    which makes it easy to compare the evaluation of the proposed solutions.

    \paragraph{Metrics.}
    Various metrics will be used to evaluate our approach.
    Search effectiveness will be evaluated using MRR@10/100/1000, since this is one of the main ways to evaluate
    models that use the MS MARCO dataset, moreover, there are already prepared scripts to help us do this.
    Metrics such as Recall@1000 and DCG@10/100/1000 will also be used.

    The following parameters will be used as additional evaluation parameters:
    \begin{enumerate}
        \item Indexing throughput;
        \item Delay when searching in the index (latency);
        \item Required space to store the index structure (disk, memory).
    \end{enumerate}

    Also, as additional measurements, it will be important to look at:
    \begin{enumerate}
        \item Dependence of quality and performance metrics on vector representation dimensions;
        \item Dependence of quality and performance metrics on the version of BERT used;
        \item Dependence of quality and performance metrics on the clustering algorithm;
        \item Dependence of quality and performance metrics on the number of clusters of contextualized
        vectors selected for clustering;
        \item Dependence of quality and performance metrics on the number of nearest neighbors used
        when searching in the HNSW index.
    \end{enumerate}

    \paragraph{Implementation.}
    In fact, at the moment, our algorithm is in the process of implementation,
    the results of which will be discussed in future papers.
    However, at the moment we have some code base and understanding about the technologies that we will use.

    To test the algorithms, we implemented IR stand, which has one interface for working
    with different information retrieval algorithms, as well as for their evaluation.
    The approach described in this paper will be added to it.
    We use the Anserini Toolkit\footnote{https://github.com/castorini/anserini} as the baseline.

    Kotlin is used to implement the stand.
    This is a modern and convenient programming language that gives access
    to the whole variety of technologies available in the JVM platform.
    In order to save computing resources, the PyTorch implementation
    of DistilBERT\footnote{https://huggingface.co/sentence-transformers/msmarco-distilbert-dot-v5}
    from the popular transformers\footnote{https://github.com/huggingface/transformers} library is used.
    Deep Java Library\footnote{https://github.com/deepjavalibrary/djl} is chosen to access DistilBERT from Kotlin code.
    The storage subsystem of the h2\footnote{http://www.h2database.com/html/mvstore.html} resident database
    was chosen as the implementation of the inverted index stored on disk.
    However, after that it was decided to switch to using the Apache Lucene\footnote{https://github.com/apache/lucene}
    library, since this is a mature technology for working with inverted indexes,
    which has shown its effectiveness, for example, in systems such
    as Elasticsearch\footnote{https://github.com/elastic/elasticsearch}.
    As an interface to the stand, at the moment, the Command Line Interface (CLI) is implemented.

    \paragraph{Hardware.}
    In the course of working with the developed stand, it was found that the indexing process using BERT
    on the CPU takes unacceptable time.
    However, the transition to the GPU and the CUDA toolkit\footnote{https://developer.nvidia.com/cuda-toolkit}
    made it possible to speed up this process many times over.
    Based on this, at the moment, the main requirement for hardware is the presence
    of a powerful GPU supported by the CUDA toolkit.

    Further hardware requirements will be refined in the course of experiments.


    \section{Further Work}
    Further work will be carried out the analysis of the proposed approach, its refinement and final implementation.
    After that, the algorithm will be evaluated according to the metrics described in \S\ref{sec:methodology}.
    When we get the values of the metrics and based on their values, it will be possible to modify
    separate parts of the architecture and evaluate the impact of changes by re-evaluating the metrics.

    As possible modifications, various clustering algorithms can be considered.

    Since not all words in the dictionary will be ambiguous, we may run into the problem that for unambiguous words,
    several contextualized vectors will be allocated that convey the same meaning.
    Therefore, when constructing clusters, it is necessary to exclude clusters that are repeated in meaning.

    So, for example, for the X-means clustering algorithm, it is possible to set the minimum and maximum
    number of clusters, after which it optimizes the number of clusters according to a given metric.

    There is also an algorithm proposed in~\cite{logachevaEtal2020Word}, which is designed
    to find contextual senses of words that differ in meaning.


    \section{Conclusion}

    In this paper we presented a possible solution to both vocabulary and token's context accounting problems in
    the information retrieval tasks.

    By using modern language models of the ``Transformer'' architecture we were able to extract contextualized vector
    representations from both documents and queries, cluster them into the finite amount of contextual meaning
    embeddings for each independent token
    and use them in order to build an inverted index and perform search queries.

    Further area of research consists of running this algorithm on a selected dataset (MS MARCO),
    evaluating different metrics search effectiveness metrics, such as MRR, and comparing obtained results with the
    existing algorithms.


    \bibliography{search-index-article}
\end{document}
