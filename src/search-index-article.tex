\documentclass[
    twocolumn,
%    hf, %enable header and footer.
]{template/ceurart}

\sloppy
\usepackage{listings}
\lstset{breaklines=true}

\begin{document}
    \copyrightyear{2022}
    \copyrightclause{Copyright for this paper by its authors.
    Use permitted under Creative Commons License Attribution 4.0
    International (CC BY 4.0).}
    \conference{This command is for the conference information}
    \title{Optimization of the structure of a full-text search index using neural networks of the "Transformer" architecture}
    \author[1]{Vyacheslav YU. Dobrynin}
    \author[1]{Roman K. Abramovich}
    \author[1]{Artem D. Gorshkov}
    \author[1]{Alexey V. Platonov}
    \address[1]{ITMO University, Kronverksky Pr. 49, bldg. A, Saint-Petersburg, 197101, Russian Federation}
    \begin{abstract}
        The use of modern language models of the ``Transformer'' architecture in information retrieval significantly
        improves its quality, but negatively affects the performance of the process.
        The paper considers an approach to the formation of an inverted search engine index,
        which makes it possible to avoid losses in search speed by precalculating the distances from dictionary
        elements to documents at the indexing stage.
    \end{abstract}
    \begin{keywords}
        Search engine \sep transformer \sep BERT \sep inverted index \sep optimization.
    \end{keywords}
    \maketitle


    \section{Introduction}
    Pre-trained neural networks allow you to effectively use complex models with less effort
    to train them to solve various kinds of problems.
    Information retrieval is no exception, in this area for a long time simple algorithms
    were used that showed high performance, but relatively low quality of search.
    The use of machine learning makes it possible to significantly improve the quality
    characteristics of the search, but due to the need to perform a large number of operations,
    i.e. high algorithmic complexity, this leads to performance losses~\cite{performanceLoss}.
    In this regard, modern search engines use two stages of retrieving relevant documents~\cite{multiStageRetrieval}.
    At the first stage, simple and effective algorithms are used that screen out most of the candidates,
    and at the second stage, neural networks are used to more accurately determine
    the relevance of the remaining documents.
    Thus, the speed of simple algorithms and the quality of complex ones are combined.
    But such an architecture has drawbacks: the documents obtained at the first stage are retrieved
    without understanding the semantics of the texts, and potentially relevant documents are skipped
    due to a vocabulary mismatch between the query and documents dictionaries~\cite{vocabularyMismatch1,vocabularyMismatch2}.
    The paper presents an approach to building an inverted index of a search engine using a neural network
    of the ``Transformer'' architecture, aimed at overcoming the above disadvantages.
    The paper also considers the solution to the problem of using polysemous words by considering their context.
    The peculiarity of the approach is that the distances from the elements of the dictionary,
    taking into account the context, to the documents are calculated at the indexing stage using the neural model,
    and the search is performed by the index without it, as a result of which the search is performed quickly
    and efficiently based on the semantics of the text.


    \section{Related Work}

    \paragraph{Vector Representation.}
    The work presented by Zamani \textit{et al.} introduced SNRM~\cite{snrm},
    an approach for mapping documents to sparse vector representations.
    This allows the use of an inverted index, resulting in high search speed and comparatively low memory consumption.
    However, in practice, it turned out that the effectiveness of SNRM is significantly
    inferior to modern state-of-the-art approaches.
    Unlike SNRM, we use dense vector BERT-based representations.
    However, they also have their limitations.
    So, for their search, nearest neighbor algorithms with linear complexity can be used,
    which is unacceptable for large amounts of data used in information retrieval.
    An acceptable solution is to use algorithms of the approximate nearest neighbor (ANN) class~\cite{ann},
    which allow a small loss of accuracy with an increase in speed by several orders of magnitude.
    Their disadvantage, depending on the specific implementation,
    is either high memory consumption or suboptimal performance.
    In this paper, we use the HNSW~\cite{hnsw} algorithm based on proximity graphs.

    \paragraph{BERT and Its Optimization.}
    To convert a document into a semantic vector space, the BERT~\cite{devlin2018bert} model is used,
    which shows the state-of-the-art results in many NLP tasks.
    However, this model is computationally expensive.
    Many researchers optimize it using distilling~\cite{bertDistilling},
    compression~\cite{bertCompression}, and pruning~\cite{bertPruning}.
    We decided to go by changing the architecture, so that BERT is not used to search for
    similarity between a query and a document online, but is used during the construction of an inverted index.
    Moreover, to speed up this process, we use DistilBERT, which is 40\% smaller, 60\% faster,
    and retains 97\% of the language understanding capabilities compared to the base BERT~\cite{bertDistilling}.

    \bibliography{search-index-article}
\end{document}
