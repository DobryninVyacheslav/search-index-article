\documentclass[
    twocolumn,
%    hf, %enable header and footer.
]{template/ceurart}

\sloppy
\usepackage{listings}
\usepackage{svg}
\usepackage{underscore} % for normal display of underscore on svg images
\lstset{breaklines=true}

\begin{document}
    \copyrightyear{2022}
    \copyrightclause{Copyright for this paper by its authors.
    Use permitted under Creative Commons License Attribution 4.0
    International (CC BY 4.0).}
    \conference{This command is for the conference information}
    \title{Optimization of the structure of a full-text search index using neural networks of the "Transformer" architecture}
    \author[1]{Vyacheslav YU. Dobrynin}
    \author[1]{Roman K. Abramovich}
    \author[1]{Artem D. Gorshkov}
    \author[1]{Alexey V. Platonov}
    \address[1]{ITMO University, Kronverksky Pr. 49, bldg. A, Saint-Petersburg, 197101, Russian Federation}
    \begin{abstract}
        The use of modern language models of the ``Transformer'' architecture in information retrieval significantly
        improves its quality, but negatively affects the performance of the process.
        The paper considers an approach to the formation of an inverted search engine index,
        which makes it possible to avoid losses in search speed by precalculating the distances from dictionary
        elements to documents at the indexing stage.

        However, standard approaches using an inverted index suffer from a vocabulary mismatch problem,
        which negatively impacts search effectiveness.
        The lack of consideration of the various meanings of words depending on the context also negatively impacts.
        To solve these problems, the paper proposes to map the senses of words, distinguishing them by contexts,
        into contextualized vector representations for storage in an inverted index and search in it.
    \end{abstract}
    \begin{keywords}
        Search engine \sep transformer \sep BERT \sep inverted index \sep optimization \sep vocabulary mismatch
        \sep word sense disambiguation.
    \end{keywords}
    \maketitle


    \section{Introduction}
    Pre-trained neural networks make it possible to effectively use complex models with less effort
    to train them to solve various kinds of problems.
    Information retrieval is no exception, in this area for a long time simple algorithms
    were used that showed high performance, but relatively low quality of search.
    The use of machine learning makes it possible to significantly improve the quality
    characteristics of the search, but due to the need to perform a large number of operations,
    i.e. high algorithmic complexity, this leads to performance losses~\cite{performanceLoss}.
    In this regard, modern search engines use two stages of retrieving relevant documents~\cite{multiStageRetrieval}.
    At the first stage, simple and effective algorithms are used that screen out most of the candidates,
    and at the second stage, neural networks are used to more accurately determine
    the relevance of the remaining documents.
    Thus, the speed of simple algorithms and the quality of complex ones are combined.
    But such an architecture has drawbacks: the documents obtained at the first stage are retrieved
    without understanding the semantics of the texts, and potentially relevant documents are skipped
    due to a vocabulary mismatch between the query and documents dictionaries~\cite{vocabularyMismatch1,vocabularyMismatch2}.

    The paper presents an approach to building an inverted index of a search engine using a neural network
    of the ``Transformer'' architecture, aimed at overcoming the above disadvantages.
    The paper also considers the solution to the problem of using polysemous words by considering their context.
    The peculiarity of the approach is that the distances from the elements of the dictionary,
    taking into account the context, to the documents are calculated at the indexing stage using the neural model,
    and the search is performed by the index without it, as a result of which the search is performed quickly
    and efficiently based on the semantics of the text.


    \section{Related Work}

    \paragraph{Vector Representation.}
    The work presented by Zamani \textit{et al.} introduced SNRM~\cite{snrm},
    an approach for mapping documents to sparse vector representations.
    This allows the use of an inverted index, resulting in high search speed and comparatively low memory consumption.
    However, in practice, it turned out that the effectiveness of SNRM is significantly
    inferior to modern state-of-the-art approaches.

    Unlike SNRM, we use dense vector BERT-based representations.
    However, they also have their limitations.
    So, for their search, nearest neighbor algorithms with linear complexity can be used,
    which is unacceptable for large amounts of data used in information retrieval.
    An acceptable solution is to use algorithms of the approximate nearest neighbor (ANN) class~\cite{ann},
    which allow a small loss of accuracy with an increase in speed by several orders of magnitude.
    Their disadvantage, depending on the specific implementation,
    is either high memory consumption or suboptimal performance.
    In this work, we use the HNSW~\cite{hnsw} algorithm based on proximity graphs.

    \paragraph{BERT and Its Optimization.}
    To convert a document into a semantic vector space, the BERT~\cite{devlin2018bert} model is used,
    which shows the state-of-the-art results in many NLP tasks.
    However, this model is computationally expensive.
    Many researchers optimize it using distilling~\cite{bertDistilling},
    compression~\cite{bertCompression}, and pruning~\cite{bertPruning}.
    We decided to go by changing the architecture, so that BERT is not used to search for
    similarity between a query and a document online, but is used during the construction of an inverted index.
    Moreover, to speed up this process, we use DistilBERT, which is 40\% smaller, 60\% faster,
    and retains 97\% of the language understanding capabilities compared to the base BERT~\cite{bertDistilling}.

    \paragraph{Contextual Word Sense Disambiguation.}
    There are many polysemous words in document and query dictionaries.
    The different meanings of a word depend on the contexts in which the word is used and
    can be distinguished with their help.
    There are methods that assign different vector representations
    to polysemous words depending on their context~\cite{athiwaratkun2018probabilistic}.
    This is also used in our algorithm, the keys of the inverted index will not be tokens,
    but the contextualized senses of these tokens.
    Thus, documents from the inverted index will be retrieved not by a specific token,
    but by its sense related to a specific context, which is meant in the query.


    \section{Inverted Index Construction}
    The proposed approach is based on using BERT to find similarities between a query and a document.
    This model helps to represent a document or a query in a multidimensional semantic space,
    using which one can find the distance or otherwise similarity between a document and a query,
    taking into account the meaning of the texts.

    However, the use of deep neural networks is accompanied by performance losses,
    which can be critical in the direct interaction of the user with the search engine.
    In view of this, a distinctive feature of the proposed approach is that it involves the use of a neural network
    only during the construction of the index.
    This is achieved due to the special structure of the inverted index,
    which makes it possible to store all the information necessary for searching.
    Also, the index is built in such a way that documents are searched for by contextualized meanings of words.
    This process is carried out in several stages.

    \subsection{Getting Contextualized Vector Representations}
    The first stage is preparatory.
    On it, for all tokens of each document obtained after preprocessing,
    windows of a given size are extracted, which are tokens and their contexts.
    So for a window length of 3 and the sentence ``ITMO University is located in Saint-Petersburg'',
    the sets will be as follows: [itmo, university], [itmo, university, locate],
    [university, locate, saint-petersburg], [locate, saint-petersburg].
    Next, the windows are passed to the BERT model to find their vector representations.
    The resulting contextualized vectors are stored in the key-value database
    along with the ID of the document to which the passed window belongs.
    Pairs (document ID, vector) for the same token are appended, not replaced.
    This completes the preparatory phase.
    Figure~\ref{fig:storeVectors} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/getting-contextualized-vectors}
        \caption{Preparation of vector representations}
        \label{fig:storeVectors}
    \end{figure}

    This stage is auxiliary and its results are necessary for the subsequent stages.

    \subsection{HNSW Index Building}
    The contextualized vectors obtained at the previous stage for each token represent its meaning in some context.
    However, it is highly likely that some vectors will represent the same meaning, and to be more precise,
    the vectors will be divided into groups in the semantic space,
    each of which will represent a specific context value of the token.
    To divide into such groups (clusters), hierarchical clustering is used in the work.
    The number of clusters equal to 16 was chosen empirically and will be refined during the experiments.
    Next, for the resulting clusters, their centroids are calculated.
    The centroid vector represents one of the contextualized token senses.
    After that, the token, the cluster identifier (which is the cluster serial number for a particular token),
    and the resulting centroid are stored in the HNSW index.
    Figure~\ref{fig:hnswIndexBuilding} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/hnsw-index-building}
        \caption{HNSW index building process}
        \label{fig:hnswIndexBuilding}
    \end{figure}

    The resulting HNSW index is needed both for building an inverted index and during an online search.

    \subsection{Inverted Index Building}
    This stage is the final one, on which the formation of the inverted index is performed.

    The vector representations of the contextual senses of the token obtained at the first stage
    are passed to the HNSW index built in the second stage to search for their nearest neighbors.
    The number of neighbors equal to 5 was chosen empirically and will be refined during the experiments.
    During the evaluation, a compromise will be chosen between the quality of the search and
    the speed of indexing and searching.
    The extracted neighbors are semantically close to the original contextual sense of the token,
    which means that they can also represent the sense of the document to which the token belongs.
    Although neighbors may contain different tokens, it is important that their semantic meaning be the same,
    as represented through embeddings.
    Thus, the vocabulary mismatch problem is solved.

    In the next step, distances are calculated between the vector representations of documents and
    the centroid vectors of the neighbors of the considered contextualized vectors.
    Distances are found using the dot product.
    The resulting scores, along with the document IDs, are written as values into an inverted index against the keys,
    which are (token, cluster ID) pairs.
    The process is repeated until the scores for all prepared contextualized vectors are indexed.
    Figure~\ref{fig:invertedIndexBuilding} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/inverted-index-building}
        \caption{Inverted index building process}
        \label{fig:invertedIndexBuilding}
    \end{figure}


    \section{Search Process}
    The search process starts in the same way as the indexing process.
    For each token of the input query obtained after preprocessing, windows of the fixed size are extracted.
    Then, each window is processed with the following algorithm:
    \begin{enumerate}
        \item Obtain vector representation by passing window to the BERT model;
        \item Get nearest neighbours from the HNSW index using obtained vector;
        \item For each pair (token, cluster ID) returned from HNSW, extract list of candidate-documents from inverted index;
        \item If a document is already present in a candidate list, merge it with the new candidate by summing their scores.
    \end{enumerate}

    At the second step of this algorithm, HNSW search can return tokens different,
    but semantically close to the original token from the query.
    Thus, the vocabulary mismatch problem is being solved both during indexing and search time.

    Next steps are no differ from the classical search algorithms.
    We sort the list of candidate-documents by their resulting score and return top-N documents as the searching result.

    Figure~\ref{fig:searchProcess} illustrates the described process.
    \begin{figure}
        \centering
        \includesvg[width=\linewidth]{image/search-process}
        \caption{Search process for a given Query}
        \label{fig:searchProcess}
    \end{figure}


    \section{Methodology}

    \paragraph{Dataset.}
    Many information retrieval works use the MS MARCO~\cite{msMarco} dataset from Microsoft,
    which was released in 2016 and adapted for retrieval in 2018.
    The dataset contains 8.8 million web page passages and 3.2 million documents that were collected
    from Bing search engine logs.
    Each query is associated with a document or multiple documents by relevancy judgments.
    The development set contains 6980 requests.

    Thus, the dataset makes it possible to solve both the passage retrieval tasks and the document retrieval tasks.
    Thanks to this, the system is evaluated on real data, which means that the evaluation
    results will more objectively show how the search engine will work in production environment.

    It is also important to note that this dataset is often used by researchers in information retrieval,
    which makes it easy to compare the evaluation of the proposed solutions.

    \paragraph{Metrics.}
    Various metrics will be used to evaluate our approach.
    Search effectiveness will be evaluated using MRR@10/100/1000, since this is one of the main ways to evaluate
    models that use the MS MARCO dataset, moreover, there are already prepared scripts to help us do this.
    Metrics such as Recall@1000 and DCG@10/100/1000 will also be used.

    The following parameters will be used as additional evaluation parameters:
    \begin{enumerate}
        \item Indexing throughput;
        \item Delay when searching in the index (latency);
        \item Required space to store the index structure (disk, memory).
    \end{enumerate}

    Also, as additional measurements, it will be important to look at:
    \begin{enumerate}
        \item Dependence of quality and performance metrics on vector representation dimensions;
        \item Dependence of quality and performance metrics on the number of clusters of contextualized
        vectors selected for clustering;
        \item Dependence of quality and performance metrics on the number of nearest neighbors used
        when searching in the HNSW index.
    \end{enumerate}

    \bibliography{search-index-article}
\end{document}
